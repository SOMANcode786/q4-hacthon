"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Introduction","href":"/q4-hacthon/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: Introduction to Physical AI","items":[{"type":"link","label":"Introduction to Physical AI","href":"/q4-hacthon/docs/module-1/introduction","docId":"module-1/introduction","unlisted":false},{"type":"link","label":"Embodied Intelligence","href":"/q4-hacthon/docs/module-1/embodied-intelligence","docId":"module-1/embodied-intelligence","unlisted":false},{"type":"link","label":"Digital-to-Physical Gap","href":"/q4-hacthon/docs/module-1/digital-to-physical-gap","docId":"module-1/digital-to-physical-gap","unlisted":false},{"type":"link","label":"Simulation-First Design","href":"/q4-hacthon/docs/module-1/simulation-first-design","docId":"module-1/simulation-first-design","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Robotic Nervous System (ROS 2)","items":[{"type":"link","label":"Introduction to ROS 2","href":"/q4-hacthon/docs/module-2/ros2-intro","docId":"module-2/ros2-intro","unlisted":false},{"type":"link","label":"Nodes, Topics, and Services","href":"/q4-hacthon/docs/module-2/nodes-topics-services","docId":"module-2/nodes-topics-services","unlisted":false},{"type":"link","label":"Python Integration with rclpy","href":"/q4-hacthon/docs/module-2/rclpy-integration","docId":"module-2/rclpy-integration","unlisted":false},{"type":"link","label":"URDF for Humanoid Robots","href":"/q4-hacthon/docs/module-2/urdf-humanoid","docId":"module-2/urdf-humanoid","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","label":"Introduction to Digital Twins","href":"/q4-hacthon/docs/module-3/digital-twin-intro","docId":"module-3/digital-twin-intro","unlisted":false},{"type":"link","label":"Gazebo Simulation Environment","href":"/q4-hacthon/docs/module-3/gazebo-simulation","docId":"module-3/gazebo-simulation","unlisted":false},{"type":"link","label":"Unity for Human-Robot Interaction","href":"/q4-hacthon/docs/module-3/unity-hri","docId":"module-3/unity-hri","unlisted":false},{"type":"link","label":"Sensor Simulation","href":"/q4-hacthon/docs/module-3/sensor-simulation","docId":"module-3/sensor-simulation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","label":"Introduction to AI Robot Brains","href":"/q4-hacthon/docs/module-4/ai-robot-brain-intro","docId":"module-4/ai-robot-brain-intro","unlisted":false},{"type":"link","label":"NVIDIA Isaac Sim","href":"/q4-hacthon/docs/module-4/isaac-sim","docId":"module-4/isaac-sim","unlisted":false},{"type":"link","label":"Isaac ROS Integration","href":"/q4-hacthon/docs/module-4/isaac-ros","docId":"module-4/isaac-ros","unlisted":false},{"type":"link","label":"Navigation 2 for Humanoids","href":"/q4-hacthon/docs/module-4/nav2-humanoid","docId":"module-4/nav2-humanoid","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 5: Vision-Language-Action Systems (VLA)","items":[{"type":"link","label":"Introduction to Vision-Language-Action Systems","href":"/q4-hacthon/docs/module-5/vla-intro","docId":"module-5/vla-intro","unlisted":false},{"type":"link","label":"Voice-to-Action Systems","href":"/q4-hacthon/docs/module-5/voice-to-action","docId":"module-5/voice-to-action","unlisted":false},{"type":"link","label":"Cognitive Planning for Robots","href":"/q4-hacthon/docs/module-5/cognitive-planning","docId":"module-5/cognitive-planning","unlisted":false},{"type":"link","label":"Vision-Guided Manipulation","href":"/q4-hacthon/docs/module-5/vision-guided-manipulation","docId":"module-5/vision-guided-manipulation","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 6: Humanoid Robotics Architecture","items":[{"type":"link","label":"Humanoid Robotics Architecture","href":"/q4-hacthon/docs/module-6/humanoid-arch-intro","docId":"module-6/humanoid-arch-intro","unlisted":false},{"type":"link","label":"Bipedal Control Systems","href":"/q4-hacthon/docs/module-6/biped-control","docId":"module-6/biped-control","unlisted":false},{"type":"link","label":"Balance and Locomotion","href":"/q4-hacthon/docs/module-6/balance-locomotion","docId":"module-6/balance-locomotion","unlisted":false},{"type":"link","label":"Perception-Action Loops","href":"/q4-hacthon/docs/module-6/perception-loops","docId":"module-6/perception-loops","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 7: RAG Chatbot Integration","items":[{"type":"link","label":"Introduction to RAG Systems","href":"/q4-hacthon/docs/module-7/rag-intro","docId":"module-7/rag-intro","unlisted":false},{"type":"link","label":"OpenAI Agents for Robotics","href":"/q4-hacthon/docs/module-7/openai-agents","docId":"module-7/openai-agents","unlisted":false},{"type":"link","label":"FastAPI Backend for Robotics","href":"/q4-hacthon/docs/module-7/fastapi-backend","docId":"module-7/fastapi-backend","unlisted":false},{"type":"link","label":"Embedded AI Assistants","href":"/q4-hacthon/docs/module-7/embedded-assistant","docId":"module-7/embedded-assistant","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 8: Capstone: The Autonomous Humanoid","items":[{"type":"link","label":"Capstone Autonomous Humanoid","href":"/q4-hacthon/docs/module-8/capstone-intro","docId":"module-8/capstone-intro","unlisted":false},{"type":"link","label":"Voice Command Workflow","href":"/q4-hacthon/docs/module-8/voice-command-workflow","docId":"module-8/voice-command-workflow","unlisted":false},{"type":"link","label":"Navigation and Detection","href":"/q4-hacthon/docs/module-8/navigation-detection","docId":"module-8/navigation-detection","unlisted":false},{"type":"link","label":"Complex Manipulation Task","href":"/q4-hacthon/docs/module-8/manipulation-task","docId":"module-8/manipulation-task","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"intro":{"id":"intro","title":"Introduction to Physical AI & Humanoid Robotics","description":"Welcome to the comprehensive guide on Physical AI & Humanoid Robotics! This book explores the fascinating intersection of artificial intelligence and physical systems, where digital intelligence meets the real world through embodied agents.","sidebar":"tutorialSidebar"},"module-1/digital-to-physical-gap":{"id":"module-1/digital-to-physical-gap","title":"Digital-to-Physical Gap","description":"Understanding the Reality Gap","sidebar":"tutorialSidebar"},"module-1/embodied-intelligence":{"id":"module-1/embodied-intelligence","title":"Embodied Intelligence","description":"Understanding Embodied Cognition","sidebar":"tutorialSidebar"},"module-1/introduction":{"id":"module-1/introduction","title":"Introduction to Physical AI","description":"What is Physical AI?","sidebar":"tutorialSidebar"},"module-1/simulation-first-design":{"id":"module-1/simulation-first-design","title":"Simulation-First Design","description":"The Philosophy of Simulation-First Development","sidebar":"tutorialSidebar"},"module-2/nodes-topics-services":{"id":"module-2/nodes-topics-services","title":"Nodes, Topics, and Services","description":"This module covers the core communication patterns in ROS 2: nodes, topics, and services that enable distributed robotic applications.","sidebar":"tutorialSidebar"},"module-2/rclpy-integration":{"id":"module-2/rclpy-integration","title":"Python Integration with rclpy","description":"This module covers how to use ROS 2 from Python using the rclpy client library, which provides a Python API for ROS 2.","sidebar":"tutorialSidebar"},"module-2/ros2-intro":{"id":"module-2/ros2-intro","title":"Introduction to ROS 2","description":"This module covers the fundamentals of ROS 2 (Robot Operating System), the middleware that provides services designed for a heterogeneous computer cluster including hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more.","sidebar":"tutorialSidebar"},"module-2/urdf-humanoid":{"id":"module-2/urdf-humanoid","title":"URDF for Humanoid Robots","description":"This module covers the Unified Robot Description Format (URDF) and how to model humanoid robots for simulation and control in ROS 2.","sidebar":"tutorialSidebar"},"module-3/digital-twin-intro":{"id":"module-3/digital-twin-intro","title":"Introduction to Digital Twins","description":"This module covers the concept of digital twins in robotics, creating virtual replicas of physical robots and environments for simulation, testing, and development.","sidebar":"tutorialSidebar"},"module-3/gazebo-simulation":{"id":"module-3/gazebo-simulation","title":"Gazebo Simulation Environment","description":"This module covers Gazebo, a physics-based simulation environment that enables accurate modeling of robot dynamics, sensors, and environments for humanoid robotics development.","sidebar":"tutorialSidebar"},"module-3/sensor-simulation":{"id":"module-3/sensor-simulation","title":"Sensor Simulation","description":"This module covers simulating various sensors in digital twin environments, including cameras, LIDAR, IMU, and other sensors critical for humanoid robot perception.","sidebar":"tutorialSidebar"},"module-3/unity-hri":{"id":"module-3/unity-hri","title":"Unity for Human-Robot Interaction","description":"This module covers using Unity as a simulation platform for humanoid robotics, particularly for developing and testing human-robot interaction scenarios.","sidebar":"tutorialSidebar"},"module-4/ai-robot-brain-intro":{"id":"module-4/ai-robot-brain-intro","title":"Introduction to AI Robot Brains","description":"This module introduces the concept of AI-powered robot brains, focusing on NVIDIA Isaac as a platform for developing intelligent robotic systems with perception, planning, and control capabilities.","sidebar":"tutorialSidebar"},"module-4/isaac-ros":{"id":"module-4/isaac-ros","title":"Isaac ROS Integration","description":"This module covers the integration between NVIDIA Isaac and ROS 2, enabling high-performance perception and AI capabilities within the ROS ecosystem.","sidebar":"tutorialSidebar"},"module-4/isaac-sim":{"id":"module-4/isaac-sim","title":"NVIDIA Isaac Sim","description":"This module covers NVIDIA Isaac Sim, a robotics simulation application based on NVIDIA Omniverse, designed for developing and testing AI-based robotic applications.","sidebar":"tutorialSidebar"},"module-4/nav2-humanoid":{"id":"module-4/nav2-humanoid","title":"Navigation 2 for Humanoids","description":"This module covers ROS 2 Navigation (Nav2) system adapted for humanoid robots, including path planning, obstacle avoidance, and locomotion control.","sidebar":"tutorialSidebar"},"module-5/cognitive-planning":{"id":"module-5/cognitive-planning","title":"Cognitive Planning for Robots","description":"This module covers cognitive planning systems that enable robots to reason about complex tasks, break them down into executable steps, and adapt to changing conditions.","sidebar":"tutorialSidebar"},"module-5/vision-guided-manipulation":{"id":"module-5/vision-guided-manipulation","title":"Vision-Guided Manipulation","description":"This module covers robotic manipulation systems that use visual feedback to guide precise movements and interactions with objects in the environment.","sidebar":"tutorialSidebar"},"module-5/vla-intro":{"id":"module-5/vla-intro","title":"Introduction to Vision-Language-Action Systems","description":"This module introduces Vision-Language-Action (VLA) systems, which combine visual perception, language understanding, and physical action to enable robots to understand and execute complex tasks based on human instructions.","sidebar":"tutorialSidebar"},"module-5/voice-to-action":{"id":"module-5/voice-to-action","title":"Voice-to-Action Systems","description":"This module covers systems that translate human voice commands into robotic actions, enabling natural human-robot interaction through speech.","sidebar":"tutorialSidebar"},"module-6/balance-locomotion":{"id":"module-6/balance-locomotion","title":"Balance and Locomotion","description":"This module covers advanced techniques for maintaining balance and achieving efficient locomotion in humanoid robots, including dynamic walking and recovery strategies.","sidebar":"tutorialSidebar"},"module-6/biped-control":{"id":"module-6/biped-control","title":"Bipedal Control Systems","description":"This module covers the control systems required for stable bipedal locomotion in humanoid robots, including balance control and gait generation.","sidebar":"tutorialSidebar"},"module-6/humanoid-arch-intro":{"id":"module-6/humanoid-arch-intro","title":"Humanoid Robotics Architecture","description":"This module covers the system architecture of humanoid robots, including hardware and software components, communication patterns, and integration strategies.","sidebar":"tutorialSidebar"},"module-6/perception-loops":{"id":"module-6/perception-loops","title":"Perception-Action Loops","description":"This module covers closed-loop control systems that integrate perception and action for real-time robotic behavior, critical for humanoid robot autonomy.","sidebar":"tutorialSidebar"},"module-7/embedded-assistant":{"id":"module-7/embedded-assistant","title":"Embedded AI Assistants","description":"This module covers the implementation of AI assistants embedded within robotic systems, enabling natural interaction and intelligent assistance capabilities.","sidebar":"tutorialSidebar"},"module-7/fastapi-backend":{"id":"module-7/fastapi-backend","title":"FastAPI Backend for Robotics","description":"This module covers building backend services for robotics applications using FastAPI, providing APIs for robot control, data management, and integration services.","sidebar":"tutorialSidebar"},"module-7/openai-agents":{"id":"module-7/openai-agents","title":"OpenAI Agents for Robotics","description":"This module covers the use of OpenAI agents in robotics applications, enabling sophisticated reasoning and planning capabilities for humanoid robots.","sidebar":"tutorialSidebar"},"module-7/rag-intro":{"id":"module-7/rag-intro","title":"Introduction to RAG Systems","description":"This module introduces Retrieval-Augmented Generation (RAG) systems in the context of robotics, enabling robots to access and utilize large knowledge bases for enhanced decision making.","sidebar":"tutorialSidebar"},"module-8/capstone-intro":{"id":"module-8/capstone-intro","title":"Capstone Autonomous Humanoid","description":"This capstone module integrates all concepts learned throughout the course to design and implement an autonomous humanoid robot system with perception, reasoning, and action capabilities.","sidebar":"tutorialSidebar"},"module-8/manipulation-task":{"id":"module-8/manipulation-task","title":"Complex Manipulation Task","description":"This module implements a complex manipulation task that demonstrates the humanoid robot\'s ability to perceive, plan, and execute precise object manipulation in unstructured environments.","sidebar":"tutorialSidebar"},"module-8/navigation-detection":{"id":"module-8/navigation-detection","title":"Navigation and Detection","description":"This module implements simultaneous navigation and object detection capabilities, enabling the humanoid robot to move safely while identifying and responding to objects in its environment.","sidebar":"tutorialSidebar"},"module-8/voice-command-workflow":{"id":"module-8/voice-command-workflow","title":"Voice Command Workflow","description":"This module implements an end-to-end workflow for processing voice commands on the humanoid robot, from speech recognition to action execution.","sidebar":"tutorialSidebar"}}}')}}]);